<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MLCA Week 7:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mike Mahoney" />
    <meta name="date" content="2021-10-13" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":true}) })</script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MLCA Week 7:
## Hyperparameter Followup
### Mike Mahoney
### 2021-10-13

---
















class: middle

# What are we doing here?

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
Some of the topics discussed fit within traditional statistics but others seem 
to have escaped, heading south, perhaps in the direction of computer science.  

The escapees were the large-scale prediction algorithms: neural nets, deep 
learning, boosting, random forests, and support-vector machines. Notably missing 
from their development were parametric probability models, the building blocks 
of classical inference. Prediction algorithms are the media stars of the 
big-data era. 
]

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
Statistics is a branch of applied mathematics, and is ultimately judged by how 
well it serves the world of applications. Mathematical logic, a la Fisher, 
has been the traditional vehicle for the development and understanding of 
statistical methods. Computation, slow and difficult before the 1950s, was only 
a bottleneck, but now has emerged as a competitor to (or perhaps an enabler of) 
mathematical analysis.
]

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
A cohesive inferential theory was forged in the first half of the twentieth 
century, but unity came at the price of an inwardly focused discipline, of 
reduced practical utility. In the century’s second half, electronic computation 
unleashed a vast expansion of useful—and much used—statistical methodology. 

Expansion accelerated at the turn of the millennium, further increasing the 
reach of statistical thinking, but now at the price of intellectual cohesion.
]


---
class: middle center

![](casi_epi.png)

---
class: middle center

&lt;img src="hyperparameter_followup_files/figure-html/unnamed-chunk-1-1.png" width="100%" /&gt;

---

class: middle

&lt;div style="font-size: 200%"&gt;

&lt;blockquote&gt;
This course attempts to guide students through several of the most common machine learning approaches at a conceptual level with a focus on applications in R.
&lt;/blockquote&gt;

&lt;/div&gt;

&lt;div style="font-size: 110%"&gt;
(From https://mlca.mm218.dev/#course-description)
&lt;/div&gt;

---

# Part 1: Prediction

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Week &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Topic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Prediction, Estimation, and Attribution &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Regression &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Classification &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Classification with imbalanced classes &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Part 2: Machine Learning

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Week &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Topic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Decision Trees &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Random Forests &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Hyperparameters and Model Tuning &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Gradient Boosting Machines &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Stochastic GBMs and Stacked Ensembles &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; k-Nearest Neighbors &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Support Vector Machines (as time allows) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Part 2: Machine Learning

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Week &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Topic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; Decision Trees &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Random Forests &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; Hyperparameters and Model Tuning &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; Gradient Boosting Machines &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Stochastic GBMs and Stacked Ensembles &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;color: grey !important;"&gt; k-Nearest Neighbors &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;font-weight: bold;"&gt; Support Vector Machines (as time allows) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Part 3: Doing The Thing

&lt;table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Week &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Topic &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 12-13 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Project Work &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Presentations &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
A great amount of ingenuity and experimentation has gone into the development of 
modern prediction algorithms, with statisticians playing an important but not 
dominant role. There is no shortage of impressive success stories. In the 
absence of optimality criteria, either frequentist or Bayesian, the prediction 
community grades algorithmic excellence on performance within a catalog of 
often-visited examples.
]

---

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
“Optimal” is the key word here. Before Fisher, statisticians didn’t really 
understand estimation. The same can be said now about prediction. Despite their 
impressive performance on a raft of test problems, it might still be possible 
to do much better than neural nets, deep learning, random forests, and boosting 
— or perhaps they are coming close to some as-yet unknown theoretical minimum.
]

---

## • There is no way to know what model will be best without trying it out*

## • There is no way to know what hyperparameters will be available without choosing a package*

## • There is no way to know what hyperparameter values will be best without fitting models*

&lt;div style="font-size: 150%"&gt;
*Though with practice you can make better guesses
&lt;/div&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
