---
title: "week_4_slides"
output: html_document
---

---
class: middle

So sensitivity and specificity, alongside ROC curves and AUC, give us a way to 
get beyond overall accuracy, so that we aren't fooled by our ~90% accurate model
that predicts 0 positives.

<br />

That said, we could only get that accuracy because there were so few positives --
employees who quit -- in our data set in the first place! 

```{r}
table(training$Attrition)
```

<br />

Our data set has almost 5 times more "No" values for attrition than "Yes", so 
models which are super accurate on
"No" and not very accurate on "Yes" can be just as "accurate" overall as a model
trying its best on both classes.

<br />

We call this an example of **imbalanced classes**, and this problem an example 
of **imbalanced classification**. 

---

We have a handful of ways of dealing with imbalanced classes. One that we've 
already talked about is to change our probability threshold using our ROC curve.

Another is to _weight_ our observations when we're fitting our model. We want
our model to care about "Yes" just as much as "No", even though there are almost
5 times as many "No" values to predict. Weights let us tell our model that every
"Yes" is "worth" 5 times as much as each "No", so that in total both classes are
"worth" the same amount to the model.

To put this into practice, we'd first want to create a "weight" column in our 
training data set. We'll set the weights of "No" values to 1 and "Yes" to 5, to
try and balance our classes:

```{r}
training <- training |> 
  mutate(weight = ifelse(Attrition, 5, 1))

training  |>
  select(Age, Attrition, weight) |> 
  head(2)
```

---

We then need to provide this new "weight" column to the "weights" argument of
`glm`:

```{r}
weighted_model <- glm(
  Attrition ~ Age, 
  training,
  weights = weight, 
  family = "binomial")
```

Then we make our predictions as we did originally -- calculate the probability 
of each employee quitting, then use a probability threshold of 0.5 to classify
employees into "Yes" and "No" groups:

```{r}
testing$prediction <- predict(weighted_model, testing, type = "response")
testing$prediction <- round(testing$prediction)
```

Let's take a look at our confusion matrix...

---

```{r}
confusionMatrix(factor(testing$prediction), 
                factor(testing$Attrition), 
                positive = "1")
```

---

One other way we might deal with imbalanced classes is to **resample** our data,
so that we have approximately equal numbers of each class in our training data.

To do that, we'd first split our training data into positive and negative pieces:

```{r}
positive_training <- training[training$Attrition == 1, ]
negative_training <- training[training$Attrition == 0, ]
```

Then we'd want to randomly select rows from our positive sample until we had the
same number of positive observations as negatives.

So long as we sample with replacement (so a row can be "selected" multiple times
in a row), this won't harm our model; we'll talk more about this procedure 
(called "**bootstrapping**") in two weeks.

We can bootstrap our positive sample to have five times the samples like this:

```{r}
n_pos <- nrow(positive_training)

bootstrapped_positives <- sample(1:n_pos, 
                                 size = 5 * n_pos, 
                                 replace = TRUE)
bootstrapped_positives <- positive_training[bootstrapped_positives, ]
```

---

Then we can combine our bootstrapped positive sample with the original negative 
sample to get a new, evenly balanced data set:

```{r}
bootstrapped_training <- rbind(
  negative_training,
  bootstrapped_positives
)
```

Just like before, we'll go ahead and fit a new model on the bootstrapped data:

```{r}
bootstrap_model <- glm(
  Attrition ~ Age, 
  bootstrapped_training,
  family = "binomial")
```

Then we make our predictions as we did originally -- calculate the probability 
of each employee quitting, then use a probability threshold of 0.5 to classify
employees into "Yes" and "No" groups:

```{r}
testing$prediction <- predict(bootstrap_model, testing, type = "response")
testing$prediction <- round(testing$prediction)
```

Let's take a look at our confusion matrix...

---

```{r}
confusionMatrix(factor(testing$prediction), 
                factor(testing$Attrition), 
                positive = "1")
```

---

Each of these approaches results in different probabilities being predicted:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(attrition_model, testing, type = "response"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(weighted_model, testing, type = "response"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(bootstrap_model, testing, type = "response"))
```

---

None of these adjustments -- using a different probability threshold, weighting
our model, or bootstrapping our sample -- can make this a _good_ model. Age just
isn't that predictive of attrition; we'd need to add other variables to get 
better results.

But all of these approaches can be used even with more complex models to try and
deal with imbalanced classes. While new methods are being proposed every day --
imbalanced classification problems are one of the most fundamental unsolved 
problems in prediction -- these three should be generally applicable to any 
real-world classification problem you have to work with.

One final note: I want to highlight that both weighting and bootstrapping used
the class abundances from the _training set_. You should not be setting weights
based on the prevalence of classes in the test set; as mentioned last week, the 
test data should be completely unknown to both the model and the modeler. That
means you shouldn't even _know_ what the class abundances are in the test set,
if you can avoid it. 

That's it for this week. Next week we leave the world of traditional statistics
and start in with our first pure prediction algorithm: the decision tree.
