---
title: "MLCA Week 3:"
subtitle: "Classification"  
author: 
  - "Mike Mahoney"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(fig.showtext = TRUE)
```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(base_color = "#23395b")
theme_set(theme_xaringan())
```

```{r xaringan-editable, echo=FALSE}
xaringanExtra::use_editable(expires = 1)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-search, echo=FALSE}
xaringanExtra::use_search(show_icon = TRUE)
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```

```{r xaringan-extra-styles, echo = FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

class: center, middle

# Classification

---
class: middle

# Many prediction problems involve predicting non-continuous outcomes.

For instance, we might want to predict:

* Whether or not an individual has a specific disease
* Which tree stems a beaver is going to harvest
* If a species will be present at a given site

---

# We can predict categorical outcomes like these using classification models.

This is in contrast to regression models, like what we talked about last week,
where we care about the numeric output of a model rather than a categorical 
classification.

Classification models might still return numeric results -- today we'll use 
models that return numeric probabilities -- but those numbers are intended to
be converted into one of a finite number of classes.

---
class: middle

Today we're going to focus specifically on **binary classification** -- all of 
our outcomes will belong to 1 of 2 categories. 

<br />

Most of our examples will focus
on predicting employee attrition (that is, predicting which employees quit or 
get fired); either an employee is still employed or they aren't, there's no 
third option.

<br />

We're focusing on this because it's easier, and because the methods we talk 
about today can't really handle more than two categories. 

<br />

But of course, there 
are plenty of times you have more than 2 categories (known as 
**multiclass problems**) -- we'll talk about methods that can handle those 
starting next week.

---

So let's walk through a classification example.

First things first, let's load our data. We'll be using the `attrition` data set
from the `modeldata` package. Our first step is to install `modeldata` if we 
haven't already:

```{r eval = FALSE}
install.packages("modeldata")
```

We then need to load the package using `library`:

```{r}
library(modeldata)
```

And last but not least, we can load the data into our R session using the `data`
function:

```{r}
data(attrition)
```

If all goes well, your data frame should have dimensions like this:

```{r}
ncol(attrition)
nrow(attrition)
```

---

This data set contains information about 1,470 employees in the IBM Watson 
Analytics lab -- what their job is, how much they made, whether they traveled, 
and, most importantly, whether or not they still work for IBM. 

That last variable is stored in the `Attrition` column, where `Yes` means the
employee left IBM and `No` means they're still there. This is what we're going 
to focus on predicting today.

We could try to model attrition as a function of an employee's age:

```{r}
attrition_model <- lm(Attrition ~ Age, attrition)
```

But we get two warnings!

---

Our warnings both mention `factors`, so let's check if any of the variables in 
the data frame are factors:

```{r}
any(
  lapply(attrition, class) == "factor"
)
```

(`lapply`, if you haven't seen it before, just runs `class` against every column
of our data frame).

Looks like we have a whole bunch of factors! Let's convert them to characters 
and try again:

```{r}
attrition_cleaned <- attrition |>
  mutate(across(where(is.factor), as.character))

try(attrition_model <- lm(Attrition ~ Age, attrition_cleaned))
```

Alright, this time we got an error! Progress!

---

The problem here is that our `Attrition` column -- the outcome/response/dependent
variable -- is stored as a character, while `lm` is expecting a number. We 
should go ahead and create a dummy variable to replace it! 

There's a somewhat more straightforward way to make dummy variables for binary
variables -- rather than use `pivot_wider` like we did last week, we can just 
manually specify replacement variables using `recode` from `dplyr`.

Let's do that and try refitting our model:

```{r}
attrition_cleaned <- attrition_cleaned |> 
  mutate(Attrition = recode(Attrition, "Yes" = 1, "No" = 0))

attrition_model <- lm(Attrition ~ Age, attrition_cleaned)
```

No warnings, no errors, we've got ourselves a model!

---

But what does this model actually mean? Let's look at it on a graph:

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
ggplot(attrition_cleaned, aes(Age, Attrition)) + 
  geom_jitter(height = 0) + 
  geom_smooth(method = "lm", formula = "y ~ x")
```

This is not exactly an intuitive graph to look at.

---

If you recall, we recoded our Attrition variable so that "Yes" (employees who quit)
were transformed to "1", and "No" was transformed to "0".

So this graph suggests that employees who quit were generally pretty young -- 
look at how the points at 1 thin out towards the older ages -- while employees
who stayed were maybe a bit older. The slope of our model seems to agree -- the
line gets closer to 0 (that is, "not quitting") as employees get older.

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
ggplot(attrition_cleaned, aes(Age, Attrition)) + 
  geom_jitter(height = 0) + 
  geom_smooth(method = "lm", formula = "y ~ x")
```

---

We can see this same relationship in the outputs of `summary` -- Age has a 
negative coefficient ("estimate"), which means that as Age goes up, the 
predicted value for Attrition goes down (by 0.006411).

```{r}
summary(attrition_model)
```

But what does it mean for Attrition to go down by 0.006411? What does the actual
number being predicted by `lm` mean?

---

The short answer is **nothing**, it means nothing.

Importantly, it is _not_ the probability of attrition; linear classifiers 
_do not calculate probability_. Imagine for example what this model would predict
for a 75 year old employee (more common at IBM than you'd think) -- what would a 
negative probability even mean?

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
ggplot(attrition_cleaned, aes(Age, Attrition)) + 
  scale_x_continuous(limits = c(10, 90)) + 
  geom_jitter(height = 0) + 
  stat_smooth(method = "lm", formula = "y ~ x", fullrange = TRUE)
```

---
class: middle

I want to stress this here, because this is a common mistake (particularly in 
economics): a linear model built to predict a binary variable does not give you 
probabilities for that binary variable.

This makes linear models really poor choices for classification problems.

So what should we do instead?

Rather than using linear classifiers, it's a better idea to use what are known 
as **logistic models** for classification problems. 

Before we get started, we should probably go ahead and create separate test and 
train data sets now:

```{r}
set.seed(123)
row_idx <- sample(seq_len(nrow(attrition_cleaned)), nrow(attrition_cleaned))
training <- attrition_cleaned[row_idx < nrow(attrition_cleaned) * 0.8, ]
testing <- attrition_cleaned[row_idx >= nrow(attrition_cleaned) * 0.8, ]
```

---

Now that we've got our training data ready, we can go ahead and fit a logistic
model. Just like before, we'll try to predict Attrition as a function of Age.
This time, though, we'll use the `glm` function instead of `lm`. To fit a 
logistic model, we also need to make sure to set `family = "binomial"`:

```{r}
attrition_model <- glm(Attrition ~ Age, training, family = "binomial")
summary(attrition_model)
```

---

Our output looks a little bit different this time!

Gone are the $R^2$ and model p-values we got from `lm`, and in their place are 
measures of deviance and AIC. 

```{r}
summary(attrition_model)
```

---

Just like with `lm`, though, we won't be paying too much attention to the 
outputs from `summary`. Instead, we want to know our model's _accuracy_.

We might expect that `predict` would give us either predicted classifications 
(so, 1s and 0s, did an employee stay or did they go) or probabilities. But if
you try using `predict` like we did with `lm`, you'll notice the predictions 
are... odd:

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
qplot(predict(attrition_model, training))
```

---

To get probabilities, we need to add the argument `type = "response"` to our
`predict` call:

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=5}
qplot(predict(attrition_model, training, type = "response"))
```

---

To calculate accuracy, we'll first need to calculate the probability of each 
employee quitting:

```{r}
testing$prediction <- predict(attrition_model, testing, type = "response")
```

We then need to convert those probabilities into predictions somehow! Probably
the easiest method is to just trust in the probabilities -- if an employee has
an over 50% chance of quitting, we'll say they quit, and if their chance is 
under 50% we'll say they didn't. 

Mechanically, this is really easy to implement: we just round our predictions so
we get 1s and 0s from the decimal probabilities:

```{r}
testing$prediction <- round(testing$prediction)
```

Our overall accuracy then is just the percentage of predictions we got right:

```{r}
sum(testing$prediction == testing$Attrition) / length(testing$Attrition)
```

89% accuracy! We did _great_!

---

You might already see the issue with this way of assessing accuracy. Our 
histogram of probabilities topped out at about 0.3 -- there wasn't a single 
employee our model gave more than a 30ish percent chance of quitting.

We can show this by replacing our predictions in the accuracy calculation with
0 -- effectively, assuming no employee would ever quit our fantastic company:

```{r}
sum(0 == testing$Attrition) / length(testing$Attrition)
```

You don't actually need a data scientist for that result.

---
class: middle

So we can see that we need more than just overall accuracy to judge our models 
by! Let's look at a few other metrics.

The `caret` package provides tools for assessing our model. Let's go ahead and
install it now:

```{r eval = FALSE}
install.packages("caret")
```

And then load it via `library`:

```{r}
library(caret)
```


---

The only `caret` function we'll use today is `confusionMatrix`. To use it, we'll
provide both our predicted values and actual values as arguments.

Note that we need to convert our predictions into factors (via `factor`). In 
this case, we also need to specify the factor levels (0 and 1); if we didn't 
specify, our predictions would only have one level (0) because we didn't predict
any 1s!

We're also going to specify which of our values should be considered the 
"positive". You should think about this term in the same way as if you were 
getting a medical test -- a "positive" result means that you have the thing the
test was for; it's generally the thing you're specifically looking for. You'll
sometimes hear this phrased as "hit" instead.

Because we're looking to figure out which employees will quit, we'll call 
"Yes" values (which we've coded as 1) "positive" here:

```{r}
attrition_confusion <- confusionMatrix(
  # Predictions go first, "true" values second:
  factor(testing$prediction, levels = 0:1),
  factor(testing$Attrition, levels = 0:1),
  # Specify what level is your "hit" or "positive" value
  positive = "1"
  )
```

---

This function creates a _lot_ of output for us to walk through:

```{r}
attrition_confusion
```

---

Starting at the top, we have a table called a **confusion matrix** (where the
function gets its name)!

```{r}
attrition_confusion$table
```

The rows represent what our model _predicted_ (either 0 or 1), while the columns
represent what it _should_ have guessed. 

Since we've called our 1s our "positives", we can see that we predicted:

+ 262 "true negatives" (prediction 0, reference 0)
+ 33 "false negatives" (prediction 0, reference 1)
+ 0 "false positives" (prediction 1, reference 0)
+ 0 "true positives" (prediction 1, reference 1)


---

Up next, we have a whole slew of accuracy metrics:

```{r}
round(attrition_confusion$overall, 4)
```

You can see "Accuracy" here represents our overall accuracy (across both classes).
We also have a 95% confidence interval ("AccuracyLower" and "AccuracyHigher"),
and the accuracy we'd get just by guessing the more common class for all of our
predictions ("AccuracyNull" -- here the same as our overall accuracy). Better
models will have a larger "Accuracy" - "AccuracyNull".

Also in this list is Cohen's Kappa ("Kappa"), which is another way to judge model
performance -- Kappa values range from -1 to 1, and higher (more positive) 
numbers are better. We won't talk about it any more because it's pretty quickly
falling out of favor -- but it pops up in enough places that it feels worthy of
calling out.

---

The next section has a lot of things worth talking about:

```{r}
round(attrition_confusion$byClass, 3)
```
The first two interesting values are the **positive and negative predictive values**
(Pos/Neg Pred Value). These represent the probabilities that a positive or 
negative prediction is a _true_ positive or negative prediction.

---

We calculate these by dividing our "true" values in the confusion matrix by the
sum of their rows:

```{r}
attrition_confusion$table
```


For example, to calculate our negative predictive power, we'd divide true 
negatives by the total number of predicted negatives:

$$\frac{262 \operatorname{true negatives}}{262 \operatorname{true negatives} + 33\operatorname{false negatives}} = \frac{262 \operatorname{true negatives}}{295 \operatorname{negatives}} = 0.888 $$

Since we don't have any positive predictions, we can't actually tell how 
accurate our positive predictions are -- 0 / (0 + 0) is undefined. 

---

Similar to predictive value are the concepts of **sensitivity** and 
**specificity**. While our predictive values tell us how likely a given _prediction_
is to be correct, sensitivity and specificity tell us how likely a given observation
is to be correctly predicted.

To be specific, **specificity** tells us what proportion of positives will be 
correctly classified as positive (the "true positive rate"), while 
**sensitivity** tells us what proportion of negatives will be classified as 
negative (the "true negative rate"). 

```{r}
round(attrition_confusion$byClass, 3)
```

---

Instead of adding up rows, we're adding up columns instead:

```{r}
attrition_confusion$table
```


Our sensitivity calculation is therefore:

$$\frac{0 \operatorname{true positives}}{0 \operatorname{true positives} + 33 \operatorname{false negatives}} = \frac{0 \operatorname{true positives}}{33\operatorname{things that should have been positive}} = 0$$

While our specificity is:

$$\frac{262 \operatorname{true negatives}}{262 \operatorname{true negatives} + 0 \operatorname{false positives}} = \frac{262 \operatorname{true negatives}}{262\operatorname{things that should have been negative}} = 1$$

Both of these values go from 0 to 1; here we've got the worst possible 
sensitivity and best possible specificity.

---

You might imagine that sensitivity and specificity are naturally opposed -- if
you set a probability threshold so that you class everything as a negative (like 
we've done here, using a probability threshold of 50%), you'll get maximum
specificity (no false positives). Similarly, if you set a threshold that classes
everything as positive (for instance, a threshold of $\geq$ 0) you could get
maximum sensitivity.

We can imagine that using different probability thresholds would give us 
different sensitivity and specificity values somewhere between the two extremes.

To look at this closer, we're going to use another new package called `pROC`. 
Install it now if you haven't before:

```{r, eval = FALSE}
install.packages("pROC")
```

And then load it with `library`:

```{r}
library(pROC)
```

---

`pROC` helps us calculate what are known as ROC curves for our classification 
models. ROC stands for "Receiver Operating Characteristic", which is a legacy
name from when they were invented to analyze radar during World War II. Everyone
just calls them ROC curves, but you'll sometimes see the full name in papers.

ROC curves let us see how using different thresholds for our model would impact
sensitivity and specificity. First, we need to create a `roc` object with the
`roc` function -- this function takes our "true" classes (Attrition in the data 
frame) and our predicted probabilities as arguments:

```{r}
attrition_roc <- roc(
  testing$Attrition,
  predict(attrition_model, testing, type = "response")
)
```

---

We can then use `plot` to see how our model trades off between sensitivity and 
specificity:

```{r, message=FALSE, warning=FALSE, fig.width=5, fig.height=5}
plot(attrition_roc)
```

Each point on the black line represents another probability threshold we could 
use, and the associated sensitivity and specificity.

---

Note that sensitivity runs the direction we normally expect -- so higher values
on the y-axis have more true positives - but specificity is backwards; values
on the left have more _true negatives_. 

This is a little confusing, so people often graph _false positives_ 
(1 - specificity) on the x-axis instead; the output graph is the exact same 
either way:

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5}
plot(attrition_roc)
```
]

.pull-right[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=5}
ggplot(coords(attrition_roc), aes(1 - specificity, sensitivity)) + 
  geom_line() + 
  labs(x = "False negative rate (1 - specificity)")
```
]

---

So what do we do with this information?

Well, it depends. We could use this to figure out what probability threshold to
use. Since the top left corner represents 100% accuracy, we might want to pick
whatever threshold gets us the closest to that point:

```{r, message=FALSE, warning=FALSE, fig.width=4, fig.height=4}
plot(attrition_roc)
```

We can use the `coords` function from pROC to find the "best" threshold this
way:

```{r}
coords(attrition_roc, "best")
```

Note that we'd normally do this using the _validation_ set, not the test set --
I'm skipping the intermediate step here to make these notes a little shorter.

---

We can test that threshold value to see how our accuracy changes:

```{r}
testing$prediction <- predict(attrition_model, testing, type = "response")
testing$prediction <- as.numeric(testing$prediction > 0.20499)

confusionMatrix(factor(testing$prediction), 
                factor(testing$Attrition), 
                positive = "1")
```

---

Our overall accuracy got a good bit worse, but we're now doing a _better_ job of
predicting positive values! So is that the threshold we should use?

Maybe. There's no rule for what an acceptable false positive/false negative rate
"should" be; it depends on your application. 

You might imagine that medical 
tests are pretty comfortable with false positives (at which point they'll test 
the patient again) if it means fewer false negatives. Conversely, if you're 
trying to predict where a species lives to prioritize 
field work, you might be willing to have more false negatives to ensure that you 
aren't wasting a bunch of time driving out to false positive sites.

This is where ROC curves really shine -- they give you a sense of what levels of
sensitivity and specificity you can get at the same time with your current 
model. If you've looked at the graph and decided what levels of false positives
and false negatives are acceptable for your current application, you can get all
the thresholds plotted on the ROC curve using `coords`:

```{r}
head(coords(attrition_roc), 2)
```

---

There's one other useful thing we can get from our ROC curve:

```{r, message=FALSE, warning=FALSE, fig.width=4, fig.height=4}
plot(attrition_roc)
```

That grey line cutting the graph in half is the ROC curve for a completely 
random model -- that is, a model that's no better than random guessing.

Our model ROC curve shows us how much better our model is than randomly guessing
at _each_ combination of sensitivity and specificity. If we want very high 
sensitivity (almost all true positives), for instance, this model isn't much
better than just guessing.

---

We can use that relationship to calculate another accuracy metric -- the 
**area under the curve (AUC)** for our model. This metric is exactly what it 
sounds like -- it's the proportion of the graph located under our model ROC
curve:

```{r, message=FALSE, warning=FALSE, fig.width=4, fig.height=4}
plot(attrition_roc, auc.polygon = TRUE)
```

Since the random model cuts the graph exactly in half, we'd expect the random 
model would have an AUC of 0.5. If the entire graph was under our curve, we'd
have an AUC of 1.0. So the closer our AUC is to 1, the closer to "perfect" our
model; the further our AUC is from 0.5, the better our model is than the 
random model.

---

We can use the `auc` function to get the actual AUC for our model:

```{r}
auc(attrition_roc)
```

The most common rule-of-thumb for AUC says:

+ If AUC == 0.5, then our model is no better than flipping a coin
+ If 0.5 $\lt$ AUC $\lt$ 0.7, the model is a "poor" classifier
+ If 0.7 $\leq$ AUC $\lt$ 0.8, the model is "acceptable"
+ If 0.8 $\leq$ AUC $\lt$ 0.9, the model is "excellent"
+ If 0.9 $\leq$ AUC, the model is "outstanding"

In general, we can assume that higher numbers are always better.

So we'd call this model pretty poor at predicting attrition -- which shouldn't
be a surprise!

---

So sensitivity and specificity, alongside ROC curves and AUC, give us a way to 
get beyond overall accuracy, so that we aren't fooled by our ~90% accurate model
that predicts 0 positives.

That said, we could only get that accuracy because there were so few positives --
employees who quit -- in our data set in the first place! 

```{r}
table(training$Attrition)
```

Our data set has almost 5 times more "No" values for attrition than "Yes", so 
models which are super accurate on
"No" and not very accurate on "Yes" can be just as "accurate" overall as a model
trying its best on both classes.

We call this an example of **imbalanced classes**, and this problem an example 
of **imbalanced classification**. 

---

We have a handful of ways of dealing with imbalanced classes. One that we've 
already talked about is to change our probability threshold using our ROC curve.

Another is to _weight_ our observations when we're fitting our model. We want
our model to care about "Yes" just as much as "No", even though there are almost
5 times as many "No" values to predict. Weights let us tell our model that every
"Yes" is "worth" 5 times as much as each "No", so that in total both classes are
"worth" the same amount to the model.

To put this into practice, we'd first want to create a "weight" column in our 
training data set. We'll set the weights of "No" values to 1 and "Yes" to 5, to
try and balance our classes:

```{r}
training <- training |> 
  mutate(weight = ifelse(Attrition, 5, 1))

training  |>
  select(Age, Attrition, weight) |> 
  head(2)
```

---

We then need to provide this new "weight" column to the "weights" argument of
`glm`:

```{r}
weighted_model <- glm(
  Attrition ~ Age, 
  training,
  weights = weight, 
  family = "binomial")
```

Then we make our predictions as we did originally -- calculate the probability 
of each employee quitting, then use a probability threshold of 0.5 to classify
employees into "Yes" and "No" groups:

```{r}
testing$prediction <- predict(weighted_model, testing, type = "response")
testing$prediction <- round(testing$prediction)
```

Let's take a look at our confusion matrix...

---

```{r}
confusionMatrix(factor(testing$prediction), 
                factor(testing$Attrition), 
                positive = "1")
```

---

One other way we might deal with imbalanced classes is to **resample** our data,
so that we have approximately equal numbers of each class in our training data.

To do that, we'd first split our training data into positive and negative pieces:

```{r}
positive_training <- training[training$Attrition == 1, ]
negative_training <- training[training$Attrition == 0, ]
```

Then we'd want to randomly select rows from our positive sample until we had the
same number of positive observations as negatives.

So long as we sample with replacement (so a row can be "selected" multiple times
in a row), this won't harm our model; we'll talk more about this procedure 
(called "**bootstrapping**") in two weeks.

We can bootstrap our positive sample to have five times the samples like this:

```{r}
n_pos <- nrow(positive_training)

bootstrapped_positives <- sample(1:n_pos, 
                                 size = 5 * n_pos, 
                                 replace = TRUE)
bootstrapped_positives <- positive_training[bootstrapped_positives, ]
```

---

Then we can combine our bootstrapped positive sample with the original negative 
sample to get a new, evenly balanced data set:

```{r}
bootstrapped_training <- rbind(
  negative_training,
  bootstrapped_positives
)
```

Just like before, we'll go ahead and fit a new model on the bootstrapped data:

```{r}
bootstrap_model <- glm(
  Attrition ~ Age, 
  bootstrapped_training,
  family = "binomial")
```

Then we make our predictions as we did originally -- calculate the probability 
of each employee quitting, then use a probability threshold of 0.5 to classify
employees into "Yes" and "No" groups:

```{r}
testing$prediction <- predict(bootstrap_model, testing, type = "response")
testing$prediction <- round(testing$prediction)
```

Let's take a look at our confusion matrix...

---

```{r}
confusionMatrix(factor(testing$prediction), 
                factor(testing$Attrition), 
                positive = "1")
```

---

Each of these approaches results in different probabilities being predicted:

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(attrition_model, testing, type = "response"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(weighted_model, testing, type = "response"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=2.5}
qplot(predict(bootstrap_model, testing, type = "response"))
```

---

None of these adjustments -- using a different probability threshold, weighting
our model, or bootstrapping our sample -- can make this a _good_ model. Age just
isn't that predictive of attrition; we'd need to add other variables to get 
better results.

But all of these approaches can be used even with more complex models to try and
deal with imbalanced classes. While new methods are being proposed every day --
imbalanced classification problems are one of the most fundamental unsolved 
problems in prediction -- these three should be generally applicable to any 
real-world classification problem you have to work with.

One final note: I want to highlight that both weighting and bootstrapping used
the class abundances from the _training set_. You should not be setting weights
based on the prevalence of classes in the test set; as mentioned last week, the 
test data should be completely unknown to both the model and the modeler. That
means you shouldn't even _know_ what the class abundances are in the test set,
if you can avoid it. 

That's it for this week. Next week we leave the world of traditional statistics
and start in with our first pure prediction algorithm: the decision tree.

---
class: middle center title-slide

## Assignment 3

(Due week 4)

---
class: middle center title-slide

# Clippings

---

These last few slides have things that didn't quite fit into this handout and 
are probably _just_ outside the scope of what we can cover in a 1-credit course.

I'm including them here for completeness; you can skip them entirely and still 
be perfectly fine for the rest of this course.

---

# Categorical loss functions

We talked about a large number of accuracy metrics earlier, from accuracy to 
predictive value to sensitivity and specificity. These are all loss functions,
just like RMSE or MAE -- numeric metrics of model performance that you can 
choose to optimize.

But these metrics are different from RMSE in an important way. Most regression
models minimize RMSE by default; the actual formula of the algorithm is written
to minimize the metric you're paying attention to.

Classification algorithms don't minimize any of the values we discussed. Instead,
they try to minimize what's known as *log loss* or *binary cross-entropy loss*
(the terms are equivalent for our purposes). This loss penalizes models based on
their _confidence_ in predictions, while all of our metrics look at the 
predictions themselves.

We won't go too far into the details here, but log loss penalizes low 
probabilities for correct predictions (and high probabilities for wrong 
predictions) much more heavily. This makes sense -- a wrong prediction the model
was 100% confident in _should be_ penalized more than wrong predictions the 
model was only 51% behind -- but is hard for humans to interpret, so we 
generally stick with our other metrics for reporting our models.

---

These are graphs of how much loss is added for different probabilities
of predictions (remembering that here lower values are better) -- you can see 
how much "confident but wrong" predictions get penalized!

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=3.5}
data.frame(
  x = seq(0.0001, 1, 0.0001),
  y = -log(seq(0.0001, 1, 0.0001))
) |> 
  ggplot(aes(x, y)) + 
  geom_line() + 
  labs(x = "predicted probability of 1",
       y = "log loss") + 
  labs(subtitle = "Log Loss When Correct Answer = 1")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=3.5}
data.frame(
  x = seq(0.0001, 1, 0.0001),
  y = -log(1 - seq(0.0001, 1, 0.0001))
) |> 
  ggplot(aes(x, y)) + 
  geom_line() + 
  labs(x = "predicted probability of 1",
       y = "log loss") + 
  labs(subtitle = "Log Loss When Correct Answer = 0")
```

---

# AUC Cut-points

Our discussion of AUC included a list of thresholds, to classify models from 
"poor" to "outstanding"

These thresholds come from [Applied Logistic Regression](https://www.wiley.com/en-us/Applied+Logistic+Regression%2C+3rd+Edition-p-9780470582473) by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant. Frustratingly, 
they are not particularly well explained in that text; instead, they're given as
magic numbers to guide interpretation. 

These thresholds are very commonly used in the literature, often citing back to
this textbook for explanation. If it appears that the thresholds just appear 
from nowhere in this handout, that's because that's how they appear to me in the
literature. 

---

# AIC

While we don't touch on it here because of our focus on accuracy metrics, I want
to mention that AIC is also a pretty good way to assess and compare models. 
AIC -- which stands for Akaike Information Criterion -- is a way of estimating 
prediction accuracy; in effect, it represents the amount of "information" 
(defined in a very specific, very mathy way) wasted by a given model. 

Models with lower AIC waste less information and are therefore better predictors;
if one model has an AIC that's at least 4 or more below another's, we say the 
first model is "better". Models with AIC within 4 of each other are assumed to 
be equivalent.

The tricky thing is that you can't compare AIC between different types of model 
-- so for instance we could use AIC to compare multiple logistic models, but not
to compare logistic models against the decision trees we'll talk about next week.
Also, AIC is a unitless value, so the actual AIC of a model doesn't give you any
information; you only really care about how a model's AIC compares to all your 
other models.

That said, AIC is still extremely common for comparing predictive models, 
especially in academia (BIC is more common for estimation models, and accuracy 
is more common in industry). If you need more information about the use of AIC,
I highly recommend Burnham and Anderson's 2004 article: [at this link](https://doi.org/10.1177%2F0049124104268644).
