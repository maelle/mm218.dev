---
title: "MLCA Week 5:"
subtitle: "Decision Trees"  
author: 
  - "Mike Mahoney"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(fig.showtext = TRUE)
```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(base_color = "#23395b")
theme_set(theme_xaringan())
```

```{r xaringan-editable, echo=FALSE}
xaringanExtra::use_editable(expires = 1)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-search, echo=FALSE}
xaringanExtra::use_search(show_icon = TRUE)
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```

```{r xaringan-extra-styles, echo = FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

class: center, middle

# Decision Trees

---
class: middle

Last week we started talking about classification using logistic regression, 
using a very simple model to predict employee attrition as a function of age.

That simple model had ~89% overall accuracy -- not because it was highly 
predictive, but simply because ~89% of employees didn't quit, so our model could
score high by assuming no one ever left.

We're going to talk this week about ways to deal with that problem.

---
class: middle

Let's start by loading packages and recreating our dataframes:

```{r, message=FALSE}
library(modeldata)
library(caret)
library(pROC)
library(rpart)
library(rpart.plot)
data(attrition)
```

<br />

And then recreate our training and testing splits using the same code as before:

```{r}
set.seed(123)
row_idx <- sample(seq_len(nrow(attrition)), nrow(attrition))
training <- attrition[row_idx < nrow(attrition) * 0.8, ]
testing <- attrition[row_idx >= nrow(attrition) * 0.8, ]
```

---

```{r}
decision_tree <- rpart(Attrition ~ TotalWorkingYears + HourlyRate, 
                       data = training)

decision_grid <- expand.grid(
  TotalWorkingYears = seq(
    min(attrition$TotalWorkingYears), 
    max(attrition$TotalWorkingYears), 
    length.out = 100),
  HourlyRate = seq(
    min(attrition$HourlyRate), 
    max(attrition$HourlyRate), 
    length.out = 100)
)

decision_grid$prediction <- predict(decision_tree, decision_grid)[, 2]

ggplot(decision_grid, aes(TotalWorkingYears, HourlyRate, fill = prediction)) + 
  geom_raster()

logmod <- glm(Attrition ~ TotalWorkingYears + HourlyRate, 
                       data = training,
              family = "binomial")

decision_grid$prediction <- predict(logmod, decision_grid, type = "response")

ggplot(decision_grid, aes(TotalWorkingYears, HourlyRate, fill = prediction)) + 
  geom_raster()
```


```{r fig.width=10, fig.height=7}
rpart(Attrition ~ TotalWorkingYears + HourlyRate, data = training) |> 
  rpart.plot(type = 4)
```

```{r}
training
```

