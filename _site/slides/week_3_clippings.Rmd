---
title: "MLCA Week 3:"
subtitle: "Classification: Clippings"  
author: 
  - "Mike Mahoney"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(ggplot2)
library(dplyr)
knitr::opts_chunk$set(fig.showtext = TRUE)
```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_light(base_color = "#23395b")
theme_set(theme_xaringan())
```

```{r xaringan-editable, echo=FALSE}
xaringanExtra::use_editable(expires = 1)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-search, echo=FALSE}
xaringanExtra::use_search(show_icon = TRUE)
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
```

```{r xaringan-extra-styles, echo = FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

class: middle center title-slide

# Clippings

---

These last few slides have things that didn't quite fit into this handout and 
are probably _just_ outside the scope of what we can cover in a 1-credit course.

I'm including them here for completeness; you can skip them entirely and still 
be perfectly fine for the rest of this course.

---

# Categorical loss functions

We talked about a large number of accuracy metrics earlier, from accuracy to 
predictive value to sensitivity and specificity. These are all loss functions,
just like RMSE or MAE -- numeric metrics of model performance that you can 
choose to optimize.

But these metrics are different from RMSE in an important way. Most regression
models minimize RMSE by default; the actual formula of the algorithm is written
to minimize the metric you're paying attention to.

Classification algorithms don't minimize any of the values we discussed. Instead,
they try to minimize what's known as *log loss* or *binary cross-entropy loss*
(the terms are equivalent for our purposes). This loss penalizes models based on
their _confidence_ in predictions, while all of our metrics look at the 
predictions themselves.

We won't go too far into the details here, but log loss penalizes low 
probabilities for correct predictions (and high probabilities for wrong 
predictions) much more heavily. This makes sense -- a wrong prediction the model
was 100% confident in _should be_ penalized more than wrong predictions the 
model was only 51% behind -- but is hard for humans to interpret, so we 
generally stick with our other metrics for reporting our models.

---

These are graphs of how much loss is added for different probabilities
of predictions (remembering that here lower values are better) -- you can see 
how much "confident but wrong" predictions get penalized!

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=3.5}
data.frame(
  x = seq(0.0001, 1, 0.0001),
  y = -log(seq(0.0001, 1, 0.0001))
) |> 
  ggplot(aes(x, y)) + 
  geom_line() + 
  labs(x = "predicted probability of 1",
       y = "log loss") + 
  labs(subtitle = "Log Loss When Correct Answer = 1")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=3.5}
data.frame(
  x = seq(0.0001, 1, 0.0001),
  y = -log(1 - seq(0.0001, 1, 0.0001))
) |> 
  ggplot(aes(x, y)) + 
  geom_line() + 
  labs(x = "predicted probability of 1",
       y = "log loss") + 
  labs(subtitle = "Log Loss When Correct Answer = 0")
```

---

# AUC Cut-points

Our discussion of AUC included a list of thresholds, to classify models from 
"poor" to "outstanding"

These thresholds come from [Applied Logistic Regression](https://www.wiley.com/en-us/Applied+Logistic+Regression%2C+3rd+Edition-p-9780470582473) by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant. Frustratingly, 
they are not particularly well explained in that text; instead, they're given as
magic numbers to guide interpretation. 

These thresholds are very commonly used in the literature, often citing back to
this textbook for explanation. If it appears that the thresholds just appear 
from nowhere in this handout, that's because that's how they appear to me in the
literature. 

---

# AIC

While we don't touch on it here because of our focus on accuracy metrics, I want
to mention that AIC is also a pretty good way to assess and compare models. 
AIC -- which stands for Akaike Information Criterion -- is a way of estimating 
prediction accuracy; in effect, it represents the amount of "information" 
(defined in a very specific, very mathy way) wasted by a given model. 

Models with lower AIC waste less information and are therefore better predictors;
if one model has an AIC that's at least 4 or more below another's, we say the 
first model is "better". Models with AIC within 4 of each other are assumed to 
be equivalent.

The tricky thing is that you can't compare AIC between different types of model 
-- so for instance we could use AIC to compare multiple logistic models, but not
to compare logistic models against the decision trees we'll talk about next week.
Also, AIC is a unitless value, so the actual AIC of a model doesn't give you any
information; you only really care about how a model's AIC compares to all your 
other models.

That said, AIC is still extremely common for comparing predictive models, 
especially in academia (BIC is more common for estimation models, and accuracy 
is more common in industry). If you need more information about the use of AIC,
I highly recommend Burnham and Anderson's 2004 article: [at this link](https://doi.org/10.1177%2F0049124104268644).
