{
  "articles": [
    {
      "path": "index.html",
      "title": "Machine Learning Concepts and Applications",
      "description": "Slides, notes, and syllabus for FOR 796.\n",
      "author": [],
      "contents": "\nCourse Title and Instructor\nTitle: FOR 796: Machine Learning Concepts and ApplicationsTime and Place: Wednesday 2:15-3:10, Bray 300\nInstructor: Mike MahoneyEmail: mike.mahoney.218@gmail.comOffice Hours: Tuesday 11:30-12:30 (Baker 231), or by appointment\nCourse Description\nPrediction has taken over the world. Whether it’s predicting how species will adapt to a changing climate, which plant cultivars will hold their own against non-native diseases, or what pair of pants a customer is most likely to buy, prediction – and the algorithms built to predict – have come to dominate how we work and live in an incredibly short span of time. These predictive methods have seen their popularity skyrocket throughout the sciences in recent years, as it becomes increasingly important to not just understand how systems work but to communicate how they might respond to human activity moving forward.\nThe algorithms developed to enable all this prediction – referred to as “pure prediction algorithms” or, more loosely, “machine learning” – have seen massive success across domains. But this success comes at the cost of complexity, and implementing these techniques requires new tools and a different mindset than traditional statistical modeling as taught to most professionals.\nThis course attempts to help bridge that gap, guiding students through several of the most common machine learning approaches at a conceptual level with a focus on applications in R. Topics include the random forest and gradient boosting machine algorithms as well as cross validation and loss functions.\nCourse Structure\nEach week there will be a handout to read before class. You are encouraged to type the code from the handouts into your own R session, in order to develop familiarity with the syntax and develop muscle memory for common tasks.\nClass time on Wednesday will be a discussion format, dedicated to answering any questions from the reading. Office hours will be available weekly to help debug code problems or answer more specific questions.\nThe course culminates in a final project where students apply concepts from the course to a data set of their choosing. The final week of class will be spent presenting results from these projects.\nLearning Objectives\nBy the end of this course, students will be able to:\nImplement common supervised learning algorithms in R to make predictions against well-formed data sets.\nUse accepted hyperparameter tuning approaches to improve model fits across multiple algorithms.\nExplain how to assess predictive models, both throughout the iteration process and as a final product.\nPrerequisites\nStudents must have some familiarity with the R language, including defining functions, managing objects, controlling the flow of a program (e.g. if/else statements and for-loops), wrangling data in data frames, fitting linear models (i.e., the lm function) and other basic tasks.\nAn introductory stats course is recommended.\nTextbooks and Materials\nThis class draws heavily from materials presented in the following two books. Both books are freely available online and you do not need to purchase a physical copy of either book to succeed in this class; there are no assigned readings from these books.\nBradley Boehmke and Brandon Greenwell. (2020). Hands on Machine Learning with R. CRC Press. Available online at https://bradleyboehmke.github.io/HOML/ .\nBradley Efron and Trevor Hastie. (2016). Computer Age Statistical Inference. Cambridge University Press. Available online at https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf .\nStudents will need to have access to a computer they are able to install R packages on.\nWhile not used in the class, students looking for a better grasp on the R programming language may benefit from the following textbooks (all freely available online):\nHadley Wickham and Garrett Grolemund. (2017). R for Data Science. O’Reilly Media, Inc. Available online at https://r4ds.had.co.nz/ .\nHadley Wickham. (2019). Advanced R. CRC Press. Available online at https://adv-r.hadley.nz/ .\nHadley Wickham and Jenny Bryan. (2021). R Packages. O’Reilly Media, Inc. Available online at https://r-pkgs.org/\nIn addition, I highly recommend the book An Introduction to Statistical Learning. While not used directly in this course, this book is potentially “the” ML textbook, and provides a comprehensible introduction to ML methods. This book is also available freely online; a citation is:\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2021). An Introduction to Statistical Learning with Applications in R. Springer Texts, second edition. Available online at https://www.statlearning.com/\nGrading\nGrades will be assigned based upon the final project. There are no other assignments in this course.\nStudents with Learning and Physical Disabilities\nSUNY-ESF works with the Office of Disability Services (ODS) at Syracuse University, who is responsible for coordinating disability-related accommodations. Students can contact ODS at 804 University Avenue- Room 309, 315-443-4498 to schedule an appointment and discuss their needs and the process for requesting accommodations. Students may also contact the ESF Office of Student Affairs, 110 Bray Hall, 315-470-6660 for assistance with the process. To learn more about ODS, visit http://disabilityservices.syr.edu. Students who attempt to use accommodations without advance notice to faculty will be referred to the ESF Office of the Dean for Student Affairs. Since accommodations may require early planning and generally are not provided retroactively, please contact ODS as soon as possible.\nAcademic Dishonesty\nAcademic dishonesty is a breach of trust between a student, one’s fellow students, or the instructor(s). By registering for courses at ESF you acknowledge your awareness of the ESF Code of Student Conduct (http://www.esf.edu/students/handbook/StudentHB.05.pdf ), in particular academic dishonesty includes but is not limited to plagiarism and cheating, and other forms of academic misconduct. The Academic Integrity Handbook contains further information and guidance (http://www.esf.edu/students/integrity/). Infractions of the academic integrity code may lead to academic penalties as per the ESF Grading Policy (http://www.esf.edu/provost/policies/documents/GradingPolicy.11.12.2013.pdf).\nInclusive Excellence Statement\nAs an institution, we embrace inclusive excellence and the strengths of a diverse and inclusive community. During classroom discussions, we may be challenged by ideas different from our lived experiences and cultures. Understanding individual differences and broader social differences will deepen our understanding of each other and the world around us. In this course, all people (including but not limited to, people of all races, ethnicities, sexual orientation, gender, gender identity and expression, students undergoing transition, religions, ages, abilities, socioeconomic backgrounds, veteran status, regions and nationalities, intellectual perspectives and political persuasion) are strongly encouraged to respectfully share their unique perspectives and experiences. This statement is intended to help cultivate a respectful environment, and it should not be used in a way that limits expression or restricts academic freedom at ESF.\n\n\n\n",
      "last_modified": "2021-11-10T15:59:02-05:00"
    },
    {
      "path": "project.html",
      "title": "FOR 796: Final Project Rubric",
      "description": "Decription and rubric for the course project for FOR 796.\n",
      "author": [],
      "contents": "\nProject Description\nThe primary goal of this course is to prepare you to apply machine learning techniques to real-world problems. This project is intended to start you in that direction.\nWorking alone, you will identify a data set, use methods from the course to create models predicting an outcome within that data set, and write a report (in the format of a short journal article).\nThis report will identify the problem you are addressing, the approach you took to tune and fit models, and the relative strengths and weaknesses of your final models. You will hand in this report as well as all the code and data used to produce it.\nFor our final class, everyone will give a brief (5 minute maximum) presentation on what they did for their report, covering their objectives, methods, results, and a reflection on how well things went and where they had issues. This should not feel like a high stakes situation, but rather an opportunity to talk with the group about the process of actually implementing machine learning methods. The presentation is worth 5% of the final grade and is scored entirely on completion (“did you give a presentation”), not quality. Slides and other presentation aids are permitted, but not required.\nYou are allowed to get help with your code from the internet, other people in the class, or myself, but the final product – code and written report – should be your own work. It’s fine to copy code snippets from the lectures or StackOverflow, but the structure and organization of your program should be your own, as should all the written report.\nAn example report (without code) is available online at https://mlca.mm218.dev/project/project_example.pdf.\nDeliverables\nYou may submit your project as a zip folder containing your data, report, and code files or as a link to a git repo of the same. The report can be a PDF, Word document, or R Markdown file (in which case the code can be either a separate R file or contained in the report itself). All projects should be submitted to mike.mahoney.218@gmail.com via an email with the subject line “FOR 796 Course Project” by the start of class on December 8th.\nFormatting should be appropriate for submitting to a journal in your field.\nRubric\nAbstract (5 points)\nShort introduction to the report that summarizes the introduction, methods, results, and discussion (5 points)\nIntroduction (7.5 points)\nExplain background of data and models, and why these models are worth building (5 points)\nClearly state objective of models (2.5 points)\nMethods (45 points)\nIntroduce the data set – number of observations and predictors, how and why it was obtained (5 points)\nAppropriately used at least one “simple” model (linear/logistic regression, decision tree, KNN) (5 points)\nAppropriately used at least two “complex” models (random forest, GBM, SVM, ensembling methods) (15 points)\nReport is reproducible: no hard-coded file paths, no setwd(), no rm(list = ls()), sets a seed, returns the same results as are reported (20 points)\nResults (15 points)\nMultiple accuracy metrics are reported appropriately (i.e., using a hold-out set) (15 points)\nFor classification, report includes overall accuracy, sensitivity, specificity, and AUC\nFor regression, report includes RMSE and MAE\n\nDiscussion (15 points)\nDiscussion of results: how do your models rank against one another? Why do you think that’s the case? (5 points)\nDiscussion of what could have been done better (5 points)\nDiscussion of what these models might be used for (5 points)\nReferences (7.5 points)\nNOTE: You do not need to do a literature review for this project. You do not need to deeply cite your introduction or discussion, especially if this isn’t data from your own work (so you don’t have the citations on-hand already). While there is nothing wrong with citing relevant work, the only required citations are:\nProper citation of your data source (2.5 points)\nProper citation of tools used (R, model packages, data wrangling packages; use the citation() function) (5 points)\nPresentation (5 points)\nPresentation included objectives, methods, results, and reflection.\nGrading\nGrades are assigned as follows:\n< 80 points: F\n80-90 points: B\n> 90 points: A\n\n\n\n",
      "last_modified": "2021-11-10T15:59:02-05:00"
    },
    {
      "path": "schedule.html",
      "title": "Schedule",
      "description": "Schedule of topics for FOR 796: Machine Learning Concepts and Applications\n",
      "author": [],
      "contents": "\n\n\nWeek\n\n\nTopic\n\n\n1\n\n\nPrediction, Estimation, and Attribution\n\n\n2\n\n\nRegression\n\n\n3\n\n\nClassification\n\n\n4\n\n\nClassification with imbalanced classes\n\n\n5\n\n\nDecision Trees\n\n\n6\n\n\nRandom Forests\n\n\n7\n\n\nHyperparameters and Model Tuning\n\n\n8\n\n\nGradient Boosting Machines\n\n\n9\n\n\nStochastic GBMs and Stacked Ensembles\n\n\n10\n\n\nk-Nearest Neighbors\n\n\n11\n\n\nSupport Vector Machines (as time allows)\n\n\n12-13\n\n\nProject Work\n\n\n14\n\n\nPresentations\n\n\nCourse Structure\nEach week there will be a handout to read before class. You are encouraged to type the code from the handouts into your own R session, in order to develop familiarity with the syntax and develop muscle memory for common tasks.\nClass time on Wednesday will be a discussion format, dedicated to answering any questions from the reading. Office hours will be available weekly to help debug code problems or answer more specific questions.\nFinal Project\nThe primary assignment for this class is a single project where you’ll apply the techniques from this course to a data set of your choosing.\nThe last week of the semester you’ll hand in two files – one containing the code used in the course of your project, the other a short report about the question you set out to answer, the methods you used, your results, and some reflection on what went well and what you’d change to make your models better. You’ll also give a five-minute presentation on your project during the final class session.\nA rubric for this project will be made available during the semester.\n\n\n\n",
      "last_modified": "2021-11-10T15:59:03-05:00"
    }
  ],
  "collections": []
}
